---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Elizabeth Son"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-elizabethson.git](https://github.com/stat301-2-2024-winter/final-project-2-elizabethson.git)

:::

## Recap of Project Memo 1:

### Overview
In Project Memo 1, I identified that this predictive modeling process will be a classification problem, aiming to predict whether a student will remain enrolled at university after two semesters, whether they will drop out, or whether they will graduate. The data comes from the UC Irvine Machine Learning Repository's collection of donated datasets and is titled "Predict students' dropout and academic success"^[UC Irvine Machine Learning Repository, "Predict students' dropout and academic success" --- [https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)].

### Progress
- I checked the data for missing values, and found that there were no missing observations across all of the 37 variables. 
- I found that there is a slight imbalance in the taret variable observations, with more observations of those who graduated compared to those who only enrolled. I accounted for this by using stratified sampling when I split the dataset into training and testing data.

## Data Splitting: (see `01_split_data.R`)
- Splits: I split the data using a proportion of 0.8 and stratified by the `target` variable.
- V-Folds: I applied V-fold cross validation to the training dataset using 10 folds and 3 repeats, also stratifying by the `target` variable. This means I will be fitting/training each model 10*3 = 30 times during the model competition/comparison stage. 

## Recipe: (see `02_recipes.R`)
Right now I have one, very basic recipe. However, as you can see in `02b_visualizing_relationships.R`, I have started to identify relationships between predictor variables to refine my recipe and create more down the road, including things like natural splines, interaction terms, and transformations. 

## Model Types:
1. Null/baseline Model: (see `03a_fit_null`)
2. Multinomial Model: (see `03b_fit_multi`)

## Assessment Metric: ROC
```{r}
#| label: tbl-roc
#| tbl-cap: "ROC of null and multinomial models"
#| echo: FALSE

# load
library(here)
load(here("results/roc_table.rda"))

roc_table
```
According to @tbl-roc, the null model worked as expected since the default area under the ROC curve is 0.5. The closer the ROC AUC is to 1.0, the greater predictive power of the model. The multinomial model has an ROC AUC value of 0.86, which is definitely closer to 1.0 and shows greater prediction accuracy compared to the null model!

## Next Steps:
- Work through `02b_visualizing_relationships.R` to create a new recipe after data explorations
- Build several other models: boosted tree, knn, elastic net, and random forest (the placeholder R scripts have been created in the `scripts/` folder)
- Potential issues: I cannot identify any potential issues for now